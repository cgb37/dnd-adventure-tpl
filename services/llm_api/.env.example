# LLM API (services/llm_api)
#
# Notes:
# - When running via `docker compose` from repo root, compose sets these env vars.
# - When running the API directly (e.g. `uvicorn llm_api.app:app`), you can copy
#   this file to `services/llm_api/.env`.

# Auth
LLM_API_KEY=dev-key

# Provider selection (the UI can override provider per request via X-LLM-Provider)
LLM_PROVIDER=ollama

# Local dev convenience
RELAX_AUTH_ON_LOCALHOST=true
# UI_ORIGIN=http://localhost:4000

# CORS allowlist (used only when RELAX_AUTH_ON_LOCALHOST=false)
# CORS_ALLOW_ORIGINS=http://localhost:4000

# Ollama (Docker should use host.docker.internal)
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.2

# Optional hosted providers (uncomment if used)
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-4.1-mini
# ANTHROPIC_API_KEY=
# ANTHROPIC_MODEL=claude-3-5-sonnet-latest
# GEMINI_API_KEY=
# GEMINI_MODEL=gemini-1.5-flash

# Limits
MAX_CONCURRENCY=4
MAX_CONCURRENCY_PER_PROVIDER=2
REQUESTS_PER_MINUTE=30
MAX_MODEL_REQUESTS_PER_GENERATION=3
MAX_OUTPUT_TOKENS=1200
MAX_REQUEST_BYTES=200000

# Logging
LOG_LEVEL=INFO
DEBUG_PROMPTS=false
